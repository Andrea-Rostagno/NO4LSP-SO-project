\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{luksan1999}
\citation{luksan1999}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{nocedal1999}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Modified Newton Method}{2}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Nelder--Mead Method}{3}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Finite Differences}{4}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Rosenbrock Function in Dimension 2}{5}{section.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Surface plot of the Rosenbrock function over the domain $[-2, 2] \times [-1, 3]$. The function exhibits a narrow curved valley with a global minimum at $(1, 1)$, where $f(x) = 0$. This geometry makes it a standard benchmark for testing unconstrained optimization algorithms.}}{5}{figure.1}\protected@file@percent }
\newlabel{fig:rosennewton}{{1}{5}{Surface plot of the Rosenbrock function over the domain $[-2, 2] \times [-1, 3]$. The function exhibits a narrow curved valley with a global minimum at $(1, 1)$, where $f(x) = 0$. This geometry makes it a standard benchmark for testing unconstrained optimization algorithms}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Convergence behaviour of the Modified Newton method on the Rosenbrock function starting from $x^{(0)} = [1.2, 1.2]$ and $x^{(0)} = [-1.2, 1.0]$.}}{6}{figure.2}\protected@file@percent }
\newlabel{fig:rosennewton}{{2}{6}{Convergence behaviour of the Modified Newton method on the Rosenbrock function starting from $x^{(0)} = [1.2, 1.2]$ and $x^{(0)} = [-1.2, 1.0]$}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Convergence behaviour of the Nelder Mead method on the Rosenbrock function starting from $x^{(0)} = [1.2, 1.2]$ and $x^{(0)} = [-1.2, 1.0]$.}}{7}{figure.3}\protected@file@percent }
\newlabel{fig:rosennewton}{{3}{7}{Convergence behaviour of the Nelder Mead method on the Rosenbrock function starting from $x^{(0)} = [1.2, 1.2]$ and $x^{(0)} = [-1.2, 1.0]$}{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Extended Rosenbrock Function}{8}{section.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces 3D visualization of the Extended Rosenbrock function in dimension $n=2$. The global minimum lies at $(1,1)$, and the function exhibits a curved valley that becomes increasingly difficult to navigate in higher dimensions.}}{8}{figure.4}\protected@file@percent }
\newlabel{fig:extrosen3d}{{4}{8}{3D visualization of the Extended Rosenbrock function in dimension $n=2$. The global minimum lies at $(1,1)$, and the function exhibits a curved valley that becomes increasingly difficult to navigate in higher dimensions}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Modified Newton with Exact Gradient and Hessian}{9}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Gradient and Hessian structure.}{9}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Motivation for sparse representation.}{9}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Experimental setup.}{10}{section*.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Convergence of the Modified Newton method with exact derivatives on the Extended Rosenbrock function for $n=1000$. Each curve corresponds to a different initial point. The method converges quadratically with stable behaviour across all tests.}}{11}{figure.5}\protected@file@percent }
\newlabel{fig:extnewton_1k}{{5}{11}{Convergence of the Modified Newton method with exact derivatives on the Extended Rosenbrock function for $n=1000$. Each curve corresponds to a different initial point. The method converges quadratically with stable behaviour across all tests}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Convergence of the Modified Newton method on the Extended Rosenbrock function for $n=10\,000$. The method exhibits consistent quadratic convergence also in higher dimension.}}{11}{figure.6}\protected@file@percent }
\newlabel{fig:extnewton_10k}{{6}{11}{Convergence of the Modified Newton method on the Extended Rosenbrock function for $n=10\,000$. The method exhibits consistent quadratic convergence also in higher dimension}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Convergence of the Modified Newton method on the Extended Rosenbrock function for $n=100\,000$. Despite the high dimensionality, the convergence remains stable with similar iteration counts.}}{12}{figure.7}\protected@file@percent }
\newlabel{fig:extnewton_100k}{{7}{12}{Convergence of the Modified Newton method on the Extended Rosenbrock function for $n=100\,000$. Despite the high dimensionality, the convergence remains stable with similar iteration counts}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Execution time (in seconds) of the Modified Newton method with exact derivatives for $n=10^3$, $10^4$ and $10^5$. The runtime grows approximately linearly with the problem size, confirming the efficiency of sparse matrix operations.}}{12}{figure.8}\protected@file@percent }
\newlabel{fig:extnewton_times}{{8}{12}{Execution time (in seconds) of the Modified Newton method with exact derivatives for $n=10^3$, $10^4$ and $10^5$. The runtime grows approximately linearly with the problem size, confirming the efficiency of sparse matrix operations}{figure.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Finite Differences Gradient and Hessian}{13}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Gradient approximation.}{13}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Hessian approximation.}{13}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Experimental results.}{14}{section*.8}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Results for $n = 1000$ using finite difference derivatives.}}{15}{table.1}\protected@file@percent }
\newlabel{tab:fd_results_1k}{{1}{15}{Results for $n = 1000$ using finite difference derivatives}{table.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Convergence of Modified Newton method on Extended Rosenbrock function ($n=1000$) with fixed increment $h = 10^{-2}$ (left) and scaled increment $h = 10^{-2}\cdot |x|$ (right).}}{16}{figure.9}\protected@file@percent }
\newlabel{fig:fd_1k_h2}{{9}{16}{Convergence of Modified Newton method on Extended Rosenbrock function ($n=1000$) with fixed increment $h = 10^{-2}$ (left) and scaled increment $h = 10^{-2}\cdot |x|$ (right)}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Convergence of Modified Newton method on Extended Rosenbrock function ($n=1000$) with fixed increment $h = 10^{-12}$ (left) and scaled increment $h = 10^{-12}\cdot |x|$ (right).}}{17}{figure.10}\protected@file@percent }
\newlabel{fig:fd_1k_h12}{{10}{17}{Convergence of Modified Newton method on Extended Rosenbrock function ($n=1000$) with fixed increment $h = 10^{-12}$ (left) and scaled increment $h = 10^{-12}\cdot |x|$ (right)}{figure.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Results for $n = 10\,000$ using finite difference derivatives.}}{18}{table.2}\protected@file@percent }
\newlabel{tab:fd_results_10k}{{2}{18}{Results for $n = 10\,000$ using finite difference derivatives}{table.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Convergence of Modified Newton method on Extended Rosenbrock function ($n=10\,000$) with fixed increment $h = 10^{-2}$ (left) and scaled increment $h = 10^{-2}\cdot |x|$ (right).}}{19}{figure.11}\protected@file@percent }
\newlabel{fig:fd_10k_h2}{{11}{19}{Convergence of Modified Newton method on Extended Rosenbrock function ($n=10\,000$) with fixed increment $h = 10^{-2}$ (left) and scaled increment $h = 10^{-2}\cdot |x|$ (right)}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Convergence of Modified Newton method on Extended Rosenbrock function ($n=10\,000$) with fixed increment $h = 10^{-12}$ (left) and scaled increment $h = 10^{-12}\cdot |x|$ (right).}}{20}{figure.12}\protected@file@percent }
\newlabel{fig:fd_10k_h12}{{12}{20}{Convergence of Modified Newton method on Extended Rosenbrock function ($n=10\,000$) with fixed increment $h = 10^{-12}$ (left) and scaled increment $h = 10^{-12}\cdot |x|$ (right)}{figure.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Results for $n = 100\,000$ using finite difference derivatives.}}{21}{table.3}\protected@file@percent }
\newlabel{tab:fd_results_100k}{{3}{21}{Results for $n = 100\,000$ using finite difference derivatives}{table.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Convergence of Modified Newton method on Extended Rosenbrock function ($n=100\,000$) with fixed increment $h = 10^{-2}$ (left) and scaled increment $h = 10^{-2}\cdot |x|$ (right).}}{22}{figure.13}\protected@file@percent }
\newlabel{fig:fd_100k_h2}{{13}{22}{Convergence of Modified Newton method on Extended Rosenbrock function ($n=100\,000$) with fixed increment $h = 10^{-2}$ (left) and scaled increment $h = 10^{-2}\cdot |x|$ (right)}{figure.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Convergence of Modified Newton method on Extended Rosenbrock function ($n=100\,000$) with fixed increment $h = 10^{-12}$ (left) and scaled increment $h = 10^{-12}\cdot |x|$ (right).}}{22}{figure.14}\protected@file@percent }
\newlabel{fig:fd_100k_h12}{{14}{22}{Convergence of Modified Newton method on Extended Rosenbrock function ($n=100\,000$) with fixed increment $h = 10^{-12}$ (left) and scaled increment $h = 10^{-12}\cdot |x|$ (right)}{figure.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Nelder–Mead Method on Extended Rosenbrock Function}{23}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Experimental setup.}{23}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Results.}{24}{section*.13}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Results of Nelder–Mead on Extended Rosenbrock function.}}{24}{table.4}\protected@file@percent }
\newlabel{tab:nelder_rosenbrock}{{4}{24}{Results of Nelder–Mead on Extended Rosenbrock function}{table.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Discussion.}{24}{section*.14}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Convergence of Nelder–Mead on Extended Rosenbrock function ($n=10$) from reference and 10 random initial points. The objective decreases but stagnates above the global minimum.}}{25}{figure.15}\protected@file@percent }
\newlabel{fig:nelder_rosen_10}{{15}{25}{Convergence of Nelder–Mead on Extended Rosenbrock function ($n=10$) from reference and 10 random initial points. The objective decreases but stagnates above the global minimum}{figure.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Convergence of Nelder–Mead on Extended Rosenbrock function ($n=26$). Progress slows down and most trajectories fail to improve after early iterations.}}{25}{figure.16}\protected@file@percent }
\newlabel{fig:nelder_rosen_26}{{16}{25}{Convergence of Nelder–Mead on Extended Rosenbrock function ($n=26$). Progress slows down and most trajectories fail to improve after early iterations}{figure.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Convergence of Nelder–Mead on Extended Rosenbrock function ($n=50$). None of the trials reach satisfactory objective values, confirming the method’s limits in high dimensions.}}{26}{figure.17}\protected@file@percent }
\newlabel{fig:nelder_rosen_50}{{17}{26}{Convergence of Nelder–Mead on Extended Rosenbrock function ($n=50$). None of the trials reach satisfactory objective values, confirming the method’s limits in high dimensions}{figure.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Execution time of Nelder–Mead on Extended Rosenbrock for increasing dimensions $n=10$, $26$, and $50$. The cost grows sharply due to the increased simplex size.}}{26}{figure.18}\protected@file@percent }
\newlabel{fig:nelder_rosen_times}{{18}{26}{Execution time of Nelder–Mead on Extended Rosenbrock for increasing dimensions $n=10$, $26$, and $50$. The cost grows sharply due to the increased simplex size}{figure.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Generalized Broyden Tridiagonal Function}{27}{section.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces 3D visualization of the Generalized Broyden Tridiagonal function for $n=2$.}}{27}{figure.19}\protected@file@percent }
\newlabel{fig:broyden3D}{{19}{27}{3D visualization of the Generalized Broyden Tridiagonal function for $n=2$}{figure.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Modified Newton Exact Gradient and Hessian}{28}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Gradient structure.}{28}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Hessian structure.}{28}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Results.}{29}{section*.17}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Results of Modified Newton method on Generalized Broyden function using exact derivatives.}}{29}{table.5}\protected@file@percent }
\newlabel{tab:gb_exact_all}{{5}{29}{Results of Modified Newton method on Generalized Broyden function using exact derivatives}{table.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Discussion.}{29}{section*.18}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Convergence of the Modified Newton method on Generalized Broyden function with $n=1000$ using exact gradient and Hessian. The method converges in few iterations with a consistent superlinear rate.}}{30}{figure.20}\protected@file@percent }
\newlabel{fig:gb_1k_exact}{{20}{30}{Convergence of the Modified Newton method on Generalized Broyden function with $n=1000$ using exact gradient and Hessian. The method converges in few iterations with a consistent superlinear rate}{figure.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Convergence of the Modified Newton method on Generalized Broyden function with $n=10000$. The convergence behavior remains stable across all random initializations.}}{31}{figure.21}\protected@file@percent }
\newlabel{fig:gb_10k_exact}{{21}{31}{Convergence of the Modified Newton method on Generalized Broyden function with $n=10000$. The convergence behavior remains stable across all random initializations}{figure.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Convergence of the Modified Newton method on Generalized Broyden function with $n=100000$. Even in high dimensions, the algorithm remains robust and fast.}}{32}{figure.22}\protected@file@percent }
\newlabel{fig:gb_100k_exact}{{22}{32}{Convergence of the Modified Newton method on Generalized Broyden function with $n=100000$. Even in high dimensions, the algorithm remains robust and fast}{figure.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Execution time of the Modified Newton method with exact derivatives on Generalized Broyden function for increasing dimensions. The computation scales efficiently due to sparse Hessian structure.}}{33}{figure.23}\protected@file@percent }
\newlabel{fig:gb_time_exact}{{23}{33}{Execution time of the Modified Newton method with exact derivatives on Generalized Broyden function for increasing dimensions. The computation scales efficiently due to sparse Hessian structure}{figure.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Newton Method Finite Differences Gradient and Hessian}{33}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Finite difference formulas.}{33}{section*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Results.}{34}{section*.20}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Finite difference results for $n=1000$ using different increments $h$ and strategies.}}{35}{table.6}\protected@file@percent }
\newlabel{tab:gb_fd_1000_all}{{6}{35}{Finite difference results for $n=1000$ using different increments $h$ and strategies}{table.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Convergence of Modified Newton method on Generalized Broyden function ($n=1000$) with fixed increment $h = 10^{-2}$ (left) and scaled increment $h = 10^{-2}\cdot |x|$ (right).}}{36}{figure.24}\protected@file@percent }
\newlabel{fig:fd_broyden_1k_h2}{{24}{36}{Convergence of Modified Newton method on Generalized Broyden function ($n=1000$) with fixed increment $h = 10^{-2}$ (left) and scaled increment $h = 10^{-2}\cdot |x|$ (right)}{figure.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Convergence of Modified Newton method on Generalized Broyden function ($n=1000$) with fixed increment $h = 10^{-12}$ (left) and scaled increment $h = 10^{-12}\cdot |x|$ (right).}}{37}{figure.25}\protected@file@percent }
\newlabel{fig:fd_broyden_1k_h12}{{25}{37}{Convergence of Modified Newton method on Generalized Broyden function ($n=1000$) with fixed increment $h = 10^{-12}$ (left) and scaled increment $h = 10^{-12}\cdot |x|$ (right)}{figure.25}{}}
\@writefile{toc}{\contentsline {paragraph}{Discussion.}{37}{section*.21}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Finite difference results for $n=10\,000$ using different increments $h$ and strategies.}}{38}{table.7}\protected@file@percent }
\newlabel{tab:gb_fd_10000}{{7}{38}{Finite difference results for $n=10\,000$ using different increments $h$ and strategies}{table.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Convergence of Modified Newton method on Generalized Broyden function ($n=10000$) with fixed increment $h = 10^{-2}$ (left) and scaled increment $h = 10^{-2}\cdot |x|$ (right).}}{39}{figure.26}\protected@file@percent }
\newlabel{fig:fd_broyden_10k_h2}{{26}{39}{Convergence of Modified Newton method on Generalized Broyden function ($n=10000$) with fixed increment $h = 10^{-2}$ (left) and scaled increment $h = 10^{-2}\cdot |x|$ (right)}{figure.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Convergence of Modified Newton method on Generalized Broyden function ($n=10000$) with fixed increment $h = 10^{-12}$ (left) and scaled increment $h = 10^{-12}\cdot |x|$ (right).}}{39}{figure.27}\protected@file@percent }
\newlabel{fig:fd_broyden_10k_h12}{{27}{39}{Convergence of Modified Newton method on Generalized Broyden function ($n=10000$) with fixed increment $h = 10^{-12}$ (left) and scaled increment $h = 10^{-12}\cdot |x|$ (right)}{figure.27}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Finite difference results for $n=100\,000$ using different increments $h$ and strategies.}}{40}{table.8}\protected@file@percent }
\newlabel{tab:gb_fd_100000}{{8}{40}{Finite difference results for $n=100\,000$ using different increments $h$ and strategies}{table.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Convergence of Modified Newton method on Generalized Broyden function ($n=100000$) with fixed increment $h = 10^{-2}$ (left) and scaled increment $h = 10^{-2}\cdot |x|$ (right).}}{41}{figure.28}\protected@file@percent }
\newlabel{fig:fd_broyden_100k_h2}{{28}{41}{Convergence of Modified Newton method on Generalized Broyden function ($n=100000$) with fixed increment $h = 10^{-2}$ (left) and scaled increment $h = 10^{-2}\cdot |x|$ (right)}{figure.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Convergence of Modified Newton method on Generalized Broyden function ($n=100000$) with fixed increment $h = 10^{-12}$ (left) and scaled increment $h = 10^{-12}\cdot |x|$ (right).}}{41}{figure.29}\protected@file@percent }
\newlabel{fig:fd_broyden_100k_h12}{{29}{41}{Convergence of Modified Newton method on Generalized Broyden function ($n=100000$) with fixed increment $h = 10^{-12}$ (left) and scaled increment $h = 10^{-12}\cdot |x|$ (right)}{figure.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Nelder–Mead Results on Generalized Broyden Function}{42}{subsection.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Results of Nelder–Mead on Generalized Broyden Tridiagonal function.}}{42}{table.9}\protected@file@percent }
\newlabel{tab:neldermead_generalized_broyden}{{9}{42}{Results of Nelder–Mead on Generalized Broyden Tridiagonal function}{table.9}{}}
\@writefile{toc}{\contentsline {paragraph}{Discussion.}{42}{section*.22}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces Convergence of Nelder-Mead method on Generalized Broyden function ($n=10$) for the reference point $\bar  {x}$ (black) and $10$ randomly generated starting points.}}{43}{figure.30}\protected@file@percent }
\newlabel{fig:gb_nelder_10}{{30}{43}{Convergence of Nelder-Mead method on Generalized Broyden function ($n=10$) for the reference point $\bar {x}$ (black) and $10$ randomly generated starting points}{figure.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces Convergence of Nelder-Mead method on Generalized Broyden function ($n=26$) for the reference point $\bar  {x}$ (black) and $10$ randomly generated starting points.}}{43}{figure.31}\protected@file@percent }
\newlabel{fig:gb_nelder_26}{{31}{43}{Convergence of Nelder-Mead method on Generalized Broyden function ($n=26$) for the reference point $\bar {x}$ (black) and $10$ randomly generated starting points}{figure.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces Convergence of Nelder-Mead method on Generalized Broyden function ($n=50$) for the reference point $\bar  {x}$ (black) and $10$ randomly generated starting points.}}{44}{figure.32}\protected@file@percent }
\newlabel{fig:gb_nelder_50}{{32}{44}{Convergence of Nelder-Mead method on Generalized Broyden function ($n=50$) for the reference point $\bar {x}$ (black) and $10$ randomly generated starting points}{figure.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces Computational time (in seconds) for Nelder-Mead method applied to the Generalized Broyden function for increasing dimensions.}}{44}{figure.33}\protected@file@percent }
\newlabel{fig:gb_nelder_time}{{33}{44}{Computational time (in seconds) for Nelder-Mead method applied to the Generalized Broyden function for increasing dimensions}{figure.33}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Banded Trigonometric Function}{45}{section.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces 3D visualization of the Banded Trigonometric function for $n=2$.}}{45}{figure.34}\protected@file@percent }
\newlabel{fig:broyden3D}{{34}{45}{3D visualization of the Banded Trigonometric function for $n=2$}{figure.34}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Exact Gradient and Hessian}{46}{subsection.5.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Results of Modified Newton method on Banded Trigonometric function using exact derivatives.}}{47}{table.10}\protected@file@percent }
\newlabel{tab:banded_exact}{{10}{47}{Results of Modified Newton method on Banded Trigonometric function using exact derivatives}{table.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {35}{\ignorespaces Convergence of the Modified Newton method on the Banded Trigonometric function ($n=1000$) using exact derivatives.}}{47}{figure.35}\protected@file@percent }
\newlabel{fig:bt_1k_exact}{{35}{47}{Convergence of the Modified Newton method on the Banded Trigonometric function ($n=1000$) using exact derivatives}{figure.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {36}{\ignorespaces Convergence of the Modified Newton method on the Banded Trigonometric function ($n=10000$) using exact derivatives.}}{48}{figure.36}\protected@file@percent }
\newlabel{fig:bt_10k_exact}{{36}{48}{Convergence of the Modified Newton method on the Banded Trigonometric function ($n=10000$) using exact derivatives}{figure.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {37}{\ignorespaces Convergence of the Modified Newton method on the Banded Trigonometric function ($n=100000$) using exact derivatives.}}{48}{figure.37}\protected@file@percent }
\newlabel{fig:bt_100k_exact}{{37}{48}{Convergence of the Modified Newton method on the Banded Trigonometric function ($n=100000$) using exact derivatives}{figure.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {38}{\ignorespaces Computational time required by the Modified Newton method on the Banded Trigonometric function with exact derivatives for each problem size.}}{49}{figure.38}\protected@file@percent }
\newlabel{fig:bt_time_exact}{{38}{49}{Computational time required by the Modified Newton method on the Banded Trigonometric function with exact derivatives for each problem size}{figure.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Finite Differences Gradient and Hessian}{49}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusions}{49}{section.6}\protected@file@percent }
\gdef \@abspage@last{51}
